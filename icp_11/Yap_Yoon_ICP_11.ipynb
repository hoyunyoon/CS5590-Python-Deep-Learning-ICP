{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0zRN7Sm9Ab1"
      },
      "source": [
        "**Remember From the Top Menu Click on Runtime -> Change Runtime type -> then Choose GPU to accelerate the learning.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCGsF9irW4Cz",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtk7nwZ4qgL4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kn23PUg9WmN3",
        "outputId": "c933fba4-722f-44b8-ba37-c21975b04612",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MNaC3jEBW9dw",
        "outputId": "58f7a966-b558-4ee3-e8d3-d94bbb904b17",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-21607a89-0030-47ff-8403-05be20f83a21\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
              "      <td>neg</td>\n",
              "      <td>0_2.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10000_4.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>test</td>\n",
              "      <td>First of all I hate those moronic rappers, who...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10001_1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10002_3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>test</td>\n",
              "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10003_3.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21607a89-0030-47ff-8403-05be20f83a21')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-21607a89-0030-47ff-8403-05be20f83a21 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-21607a89-0030-47ff-8403-05be20f83a21');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  type                                             review label  \\\n",
              "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
              "1           1  test  This is an example of why the majority of acti...   neg   \n",
              "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
              "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
              "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
              "\n",
              "          file  \n",
              "0      0_2.txt  \n",
              "1  10000_4.txt  \n",
              "2  10001_1.txt  \n",
              "3  10002_3.txt  \n",
              "4  10003_3.txt  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/imdb_master.csv',encoding='latin-1')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aX6mS_1wOpA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "sentences = df['review'].values\n",
        "y = df['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnG-6A6VpsN1",
        "outputId": "737e2a83-84ba-450d-a93c-487dc7b080d1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['neg' 25000]\n",
            " ['pos' 25000]\n",
            " ['unsup' 50000]]\n"
          ]
        }
      ],
      "source": [
        "(unique, counts) = np.unique(y, return_counts=True)\n",
        "\n",
        "frequencies = np.asarray((unique, counts)).T\n",
        "\n",
        "print(frequencies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df6hooweXBJo"
      },
      "source": [
        "tokenizing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSUceLgkW-wI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=2000)\n",
        "tokenizer.fit_on_texts(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR78DrA6XD48"
      },
      "source": [
        "getting the vocabulary of data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqW0hpE5beji",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Transforms each text in texts to a sequence of integers.\n",
        "# Only top num_words-1 most frequent words will be taken into account. Only words known by the tokenizer will be taken into account.\n",
        "\n",
        "# Transforms each text in texts to a sequence of integers. \n",
        "# So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n",
        "X = tokenizer.texts_to_sequences(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_Hk__8Jbk0n",
        "outputId": "4f081c30-383d-4cd4-b92f-76e7ddd1a832",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Len() of X: 100000\n",
            "\n",
            " [[281, 173, 466, 45, 42, 3, 17, 16, 229, 1138, 71, 1700, 1185, 35, 1, 1344, 1610, 829, 4, 60, 47, 23, 52, 171, 10, 39, 118, 22, 459, 41, 100, 4, 1, 101, 88, 4, 175, 25, 8, 1, 2, 106, 23, 1710, 408, 20, 2, 94, 1518, 358, 72, 308, 31, 60, 55, 10, 118, 22, 459, 1, 106, 73, 141, 64, 459, 41, 6, 3, 52, 1, 440, 6, 28, 272, 120, 14, 554, 34, 1207, 228, 125, 71, 255, 334, 183, 86, 2, 275, 54, 4, 3, 24, 62, 731, 5, 26, 1763, 121, 431, 50, 73, 23, 69, 504, 1, 310, 95, 223, 4, 11, 710, 175, 29, 41, 73, 23, 579, 134, 6, 5, 26, 1, 116, 15, 54, 38, 54, 1326, 133, 9, 13, 29, 10, 97, 78, 5, 386, 35, 1591, 9, 120, 32, 542, 8], [11, 6, 32, 496, 4, 134, 1, 4, 188, 103, 23, 1, 168, 2, 345, 208, 64, 161, 274, 147, 133, 3, 610, 454, 4, 1, 94, 1184, 4, 1784, 818, 2, 1784, 251, 107, 211, 121, 12, 33, 23, 4, 114, 2, 114, 69, 89, 1300, 15, 11, 27, 138, 65, 164, 698, 510, 38, 104, 164, 804, 16, 1784, 818, 38, 1, 38, 16, 1784, 2, 65, 1, 143, 816, 1784, 402, 633, 162, 11, 19, 30, 1, 2, 146, 131, 1568, 48, 1, 979, 13, 399, 8, 11, 19, 2, 134, 1, 124, 28, 212, 296, 1, 168, 106, 35, 1711, 170, 19, 203, 109, 15, 979, 45, 86, 394, 1, 168, 106, 2, 30, 221, 8, 1711, 24, 106, 1092, 60, 91, 9, 655, 7, 7, 436, 11, 6, 326, 991, 188, 1217, 47, 23, 125, 103, 5, 65, 2, 44, 21, 64, 181, 5, 65, 11, 27, 104, 311, 60, 6, 3, 982, 18, 45, 125, 114, 2, 3, 125, 226, 1, 62, 148, 12, 91, 11, 30, 29, 274, 147, 13, 3, 529, 506, 20, 1, 356, 1, 652, 13, 219, 60, 272, 501, 5, 234, 53, 16, 1, 490, 19, 390, 18, 22, 180, 453, 158]]\n"
          ]
        }
      ],
      "source": [
        "print('\\nLen() of X:', len(X))\n",
        "print('\\n', X[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T885TTUkbpGo",
        "outputId": "6fb3e5ed-a5be-499a-ff0d-e55a55511f47",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X.shape =  (100000, 1945)\n"
          ]
        }
      ],
      "source": [
        "X = pad_sequences(X) # Pads sequences to the same length.\n",
        "print('X.shape = ', X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuJJ6l52XHTw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "le = preprocessing.LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7ig6dd14w8Q",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8u2v7cLXNQQ",
        "outputId": "da95a2b7-da76-4745-c1f4-bff97395971d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(75000, 1945)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcyQwlRNXI6V",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "# changed input_dim to equal 1945 to allow the code to run\n",
        "input_dim = X_train.shape[1]\n",
        "model.add(layers.Dense(300,input_dim=input_dim, activation='relu'))\n",
        "model.add(layers.Dense(5, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PP5EQL0BYSzq",
        "outputId": "6608712d-b673-4900-e94a-0c299580f765",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 300)               583800    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 1505      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 585,305\n",
            "Trainable params: 585,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGpcm3BIXNmL",
        "outputId": "fca6e227-458b-482b-a041-d33abc1a9845",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: 6.0980 - acc: 0.4460 - val_loss: 1.1359 - val_acc: 0.4931\n",
            "Epoch 2/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0865 - acc: 0.4918 - val_loss: 1.1277 - val_acc: 0.4914\n",
            "Epoch 3/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.1314 - acc: 0.4983 - val_loss: 1.1576 - val_acc: 0.4971\n",
            "Epoch 4/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0605 - acc: 0.4999 - val_loss: 1.0593 - val_acc: 0.4980\n",
            "Epoch 5/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: 1.0554 - acc: 0.5007 - val_loss: 1.0655 - val_acc: 0.4980\n",
            "Epoch 6/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0808 - acc: 0.5009 - val_loss: 1.0600 - val_acc: 0.4986\n",
            "Epoch 7/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0476 - acc: 0.5016 - val_loss: 1.0475 - val_acc: 0.4988\n",
            "Epoch 8/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0399 - acc: 0.5018 - val_loss: 1.0502 - val_acc: 0.4983\n",
            "Epoch 9/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0426 - acc: 0.5018 - val_loss: 1.0471 - val_acc: 0.4986\n",
            "Epoch 10/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0378 - acc: 0.5019 - val_loss: 1.0521 - val_acc: 0.4988\n",
            "Epoch 11/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0377 - acc: 0.5021 - val_loss: 1.0511 - val_acc: 0.4988\n",
            "Epoch 12/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0372 - acc: 0.5023 - val_loss: 1.0504 - val_acc: 0.4987\n",
            "Epoch 13/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0377 - acc: 0.5023 - val_loss: 1.0531 - val_acc: 0.4985\n",
            "Epoch 14/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0374 - acc: 0.5023 - val_loss: 1.0529 - val_acc: 0.4986\n",
            "Epoch 15/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0382 - acc: 0.5023 - val_loss: 1.0525 - val_acc: 0.4983\n",
            "Epoch 16/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0394 - acc: 0.5020 - val_loss: 1.0506 - val_acc: 0.4986\n",
            "Epoch 17/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0391 - acc: 0.5020 - val_loss: 1.0525 - val_acc: 0.4988\n",
            "Epoch 18/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0382 - acc: 0.5020 - val_loss: 1.0591 - val_acc: 0.4983\n",
            "Epoch 19/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0381 - acc: 0.5023 - val_loss: 1.0671 - val_acc: 0.4986\n",
            "Epoch 20/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0374 - acc: 0.5021 - val_loss: 1.0517 - val_acc: 0.4986\n",
            "Epoch 21/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0398 - acc: 0.5021 - val_loss: 1.0599 - val_acc: 0.4986\n",
            "Epoch 22/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0371 - acc: 0.5022 - val_loss: 1.0558 - val_acc: 0.4989\n",
            "Epoch 23/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0367 - acc: 0.5022 - val_loss: 1.0576 - val_acc: 0.4988\n",
            "Epoch 24/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0367 - acc: 0.5021 - val_loss: 1.0574 - val_acc: 0.4986\n",
            "Epoch 25/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0368 - acc: 0.5022 - val_loss: 1.0565 - val_acc: 0.4986\n",
            "Epoch 26/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0372 - acc: 0.5024 - val_loss: 1.0634 - val_acc: 0.4989\n",
            "Epoch 27/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0372 - acc: 0.5024 - val_loss: 1.0556 - val_acc: 0.4989\n",
            "Epoch 28/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0372 - acc: 0.5023 - val_loss: 1.0565 - val_acc: 0.4986\n",
            "Epoch 29/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0365 - acc: 0.5023 - val_loss: 1.0758 - val_acc: 0.4983\n",
            "Epoch 30/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0374 - acc: 0.5023 - val_loss: 1.0547 - val_acc: 0.4987\n",
            "Epoch 31/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0375 - acc: 0.5023 - val_loss: 1.0574 - val_acc: 0.4989\n",
            "Epoch 32/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0372 - acc: 0.5023 - val_loss: 1.0653 - val_acc: 0.4990\n",
            "Epoch 33/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0363 - acc: 0.5024 - val_loss: 1.0607 - val_acc: 0.4990\n",
            "Epoch 34/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0364 - acc: 0.5023 - val_loss: 1.0579 - val_acc: 0.4989\n",
            "Epoch 35/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0365 - acc: 0.5024 - val_loss: 1.0623 - val_acc: 0.4990\n",
            "Epoch 36/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0366 - acc: 0.5024 - val_loss: 1.0670 - val_acc: 0.4987\n",
            "Epoch 37/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0367 - acc: 0.5025 - val_loss: 1.0659 - val_acc: 0.4987\n",
            "Epoch 38/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0377 - acc: 0.5025 - val_loss: 1.0751 - val_acc: 0.4986\n",
            "Epoch 39/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0367 - acc: 0.5024 - val_loss: 1.0658 - val_acc: 0.4987\n",
            "Epoch 40/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0356 - acc: 0.5025 - val_loss: 1.0588 - val_acc: 0.4984\n",
            "Epoch 41/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0358 - acc: 0.5026 - val_loss: 1.0653 - val_acc: 0.4984\n",
            "Epoch 42/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0358 - acc: 0.5026 - val_loss: 1.0644 - val_acc: 0.4986\n",
            "Epoch 43/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0357 - acc: 0.5026 - val_loss: 1.0627 - val_acc: 0.4985\n",
            "Epoch 44/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0358 - acc: 0.5025 - val_loss: 1.0613 - val_acc: 0.4986\n",
            "Epoch 45/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0354 - acc: 0.5026 - val_loss: 1.0734 - val_acc: 0.4985\n",
            "Epoch 46/50\n",
            "2344/2344 [==============================] - 10s 4ms/step - loss: 1.0352 - acc: 0.5026 - val_loss: 1.0836 - val_acc: 0.4985\n",
            "Epoch 47/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0366 - acc: 0.5027 - val_loss: 1.0684 - val_acc: 0.4984\n",
            "Epoch 48/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0553 - acc: 0.5027 - val_loss: 1.0859 - val_acc: 0.4985\n",
            "Epoch 49/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0451 - acc: 0.5026 - val_loss: 1.0644 - val_acc: 0.4986\n",
            "Epoch 50/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: 1.0363 - acc: 0.5026 - val_loss: 1.0772 - val_acc: 0.4986\n"
          ]
        }
      ],
      "source": [
        "history=model.fit(X_train,y_train, epochs=50, verbose=True, validation_data=(X_test,y_test), batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iGgnNNQGiPd"
      },
      "source": [
        "Martin Yap, Hoyun Yoon, ICP 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9BYwqeOK2kS"
      },
      "source": [
        "1) Identify the mistakes in the model code and explain why they need to be corrected to be able to get the code to run."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2LtOUEGLNEC"
      },
      "source": [
        "The first mistake in the code is that it is using a variable named input_dim that is not defined. This needs to be corrected since the variable is being used to define the input dimension of the model. This mistake can be corrected by either defining input_dim or hard coding the dimension into the input_dim parameter. In the case for this model, we want the input_dim to be 1945 since that is the dimension defined using the .shape method with the training dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pjvrM9NM4a2"
      },
      "source": [
        "The next mistake is the number of neurons used in the last layer for the model. The current model uses 5 neurons for the last layer. However, the model should be using one neuron for each class it is identifying. This needs to be corrected since the model would be making classifications for classes that are not applicable to the dataset, which would result in a low accuracy for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omo9G5wDRC4o"
      },
      "source": [
        "For the next mistake in the model code, we would need to change the activation function for the last layer. The current model code uses the softmax activation function, which is used when more than two classes are present in the classification. The code needs to be corrected since the model is mainly looking at two classifications. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MFj2JG5RTuG"
      },
      "source": [
        "The last mistake in the model code is the loss method being used. The current model code is using the sparse_categorical_crossentropy loss method. This needs to be corrected since sparse categorical crossentropy is used when classifying multiple classes, the labels are one-hot encoded, and we are just identifying if the review is positive or negative.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvWTSZSrALHV",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model1 = Sequential()\n",
        "\n",
        "# Define input dimension\n",
        "input_dim = X_train.shape[1]\n",
        "model1.add(layers.Dense(300,input_dim=input_dim, activation='relu'))\n",
        "\n",
        "# Change the output layer neurons and activation function\n",
        "model1.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Change the loss function\n",
        "model1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsrdlZeKAZ5o",
        "outputId": "9b4d2e58-ec21-437c-ec7f-493814882690",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "2344/2344 [==============================] - 15s 5ms/step - loss: -3845293.7500 - acc: 0.2496 - val_loss: -11004955.0000 - val_acc: 0.2510\n",
            "Epoch 2/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: -23359832.0000 - acc: 0.2497 - val_loss: -37884848.0000 - val_acc: 0.2510\n",
            "Epoch 3/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -56292716.0000 - acc: 0.2497 - val_loss: -76593440.0000 - val_acc: 0.2510\n",
            "Epoch 4/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -100662200.0000 - acc: 0.2497 - val_loss: -126550816.0000 - val_acc: 0.2510\n",
            "Epoch 5/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -155987776.0000 - acc: 0.2497 - val_loss: -187192608.0000 - val_acc: 0.2510\n",
            "Epoch 6/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -222226880.0000 - acc: 0.2497 - val_loss: -258872736.0000 - val_acc: 0.2510\n",
            "Epoch 7/50\n",
            "2344/2344 [==============================] - 13s 5ms/step - loss: -299322944.0000 - acc: 0.2497 - val_loss: -341257504.0000 - val_acc: 0.2510\n",
            "Epoch 8/50\n",
            "2344/2344 [==============================] - 13s 5ms/step - loss: -387087968.0000 - acc: 0.2497 - val_loss: -434558912.0000 - val_acc: 0.2510\n",
            "Epoch 9/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -485609184.0000 - acc: 0.2497 - val_loss: -537905920.0000 - val_acc: 0.2510\n",
            "Epoch 10/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: -594172736.0000 - acc: 0.2497 - val_loss: -651366080.0000 - val_acc: 0.2510\n",
            "Epoch 11/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -713224960.0000 - acc: 0.2497 - val_loss: -776112832.0000 - val_acc: 0.2510\n",
            "Epoch 12/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: -843622272.0000 - acc: 0.2497 - val_loss: -911926848.0000 - val_acc: 0.2510\n",
            "Epoch 13/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -984256384.0000 - acc: 0.2497 - val_loss: -1057069184.0000 - val_acc: 0.2510\n",
            "Epoch 14/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -1134334592.0000 - acc: 0.2497 - val_loss: -1212352128.0000 - val_acc: 0.2510\n",
            "Epoch 15/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: -1295127296.0000 - acc: 0.2497 - val_loss: -1378376192.0000 - val_acc: 0.2510\n",
            "Epoch 16/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -1466625792.0000 - acc: 0.2497 - val_loss: -1555020288.0000 - val_acc: 0.2510\n",
            "Epoch 17/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -1647913984.0000 - acc: 0.2497 - val_loss: -1741203584.0000 - val_acc: 0.2510\n",
            "Epoch 18/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: -1839850240.0000 - acc: 0.2497 - val_loss: -1937878400.0000 - val_acc: 0.2510\n",
            "Epoch 19/50\n",
            "2344/2344 [==============================] - 13s 6ms/step - loss: -2041884032.0000 - acc: 0.2497 - val_loss: -2145514112.0000 - val_acc: 0.2510\n",
            "Epoch 20/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -2254464256.0000 - acc: 0.2497 - val_loss: -2362573312.0000 - val_acc: 0.2510\n",
            "Epoch 21/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -2477696000.0000 - acc: 0.2497 - val_loss: -2591220736.0000 - val_acc: 0.2510\n",
            "Epoch 22/50\n",
            "2344/2344 [==============================] - 13s 6ms/step - loss: -2711652352.0000 - acc: 0.2497 - val_loss: -2830430464.0000 - val_acc: 0.2510\n",
            "Epoch 23/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -2956396544.0000 - acc: 0.2497 - val_loss: -3080402432.0000 - val_acc: 0.2510\n",
            "Epoch 24/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -3210461952.0000 - acc: 0.2497 - val_loss: -3337960704.0000 - val_acc: 0.2510\n",
            "Epoch 25/50\n",
            "2344/2344 [==============================] - 13s 5ms/step - loss: -3472278016.0000 - acc: 0.2497 - val_loss: -3605007872.0000 - val_acc: 0.2510\n",
            "Epoch 26/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -3745872896.0000 - acc: 0.2497 - val_loss: -3884507392.0000 - val_acc: 0.2510\n",
            "Epoch 27/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -4030823680.0000 - acc: 0.2497 - val_loss: -4174225664.0000 - val_acc: 0.2510\n",
            "Epoch 28/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -4326601216.0000 - acc: 0.2497 - val_loss: -4474803200.0000 - val_acc: 0.2510\n",
            "Epoch 29/50\n",
            "2344/2344 [==============================] - 13s 6ms/step - loss: -4630820864.0000 - acc: 0.2497 - val_loss: -4783610880.0000 - val_acc: 0.2510\n",
            "Epoch 30/50\n",
            "2344/2344 [==============================] - 13s 5ms/step - loss: -4945744384.0000 - acc: 0.2497 - val_loss: -5103853056.0000 - val_acc: 0.2510\n",
            "Epoch 31/50\n",
            "2344/2344 [==============================] - 13s 5ms/step - loss: -5270647296.0000 - acc: 0.2497 - val_loss: -5433822720.0000 - val_acc: 0.2510\n",
            "Epoch 32/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -5606850048.0000 - acc: 0.2497 - val_loss: -5774671872.0000 - val_acc: 0.2510\n",
            "Epoch 33/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -5951605760.0000 - acc: 0.2497 - val_loss: -6123842048.0000 - val_acc: 0.2510\n",
            "Epoch 34/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -6306248704.0000 - acc: 0.2497 - val_loss: -6484581888.0000 - val_acc: 0.2510\n",
            "Epoch 35/50\n",
            "2344/2344 [==============================] - 13s 6ms/step - loss: -6673699840.0000 - acc: 0.2497 - val_loss: -6855514112.0000 - val_acc: 0.2510\n",
            "Epoch 36/50\n",
            "2344/2344 [==============================] - 13s 6ms/step - loss: -7047516672.0000 - acc: 0.2497 - val_loss: -7234487808.0000 - val_acc: 0.2510\n",
            "Epoch 37/50\n",
            "2344/2344 [==============================] - 13s 5ms/step - loss: -7433213952.0000 - acc: 0.2497 - val_loss: -7625126400.0000 - val_acc: 0.2510\n",
            "Epoch 38/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -7829793280.0000 - acc: 0.2497 - val_loss: -8027213824.0000 - val_acc: 0.2510\n",
            "Epoch 39/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -8236166144.0000 - acc: 0.2497 - val_loss: -8437490688.0000 - val_acc: 0.2510\n",
            "Epoch 40/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -8652902400.0000 - acc: 0.2497 - val_loss: -8858556416.0000 - val_acc: 0.2510\n",
            "Epoch 41/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -9080176640.0000 - acc: 0.2497 - val_loss: -9292228608.0000 - val_acc: 0.2510\n",
            "Epoch 42/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -9517811712.0000 - acc: 0.2497 - val_loss: -9734659072.0000 - val_acc: 0.2510\n",
            "Epoch 43/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -9965742080.0000 - acc: 0.2497 - val_loss: -10186160128.0000 - val_acc: 0.2510\n",
            "Epoch 44/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -10423394304.0000 - acc: 0.2497 - val_loss: -10648154112.0000 - val_acc: 0.2510\n",
            "Epoch 45/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: -10888435712.0000 - acc: 0.2497 - val_loss: -11118660608.0000 - val_acc: 0.2510\n",
            "Epoch 46/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -11365744640.0000 - acc: 0.2497 - val_loss: -11600506880.0000 - val_acc: 0.2510\n",
            "Epoch 47/50\n",
            "2344/2344 [==============================] - 11s 5ms/step - loss: -11853721600.0000 - acc: 0.2497 - val_loss: -12094449664.0000 - val_acc: 0.2510\n",
            "Epoch 48/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -12353157120.0000 - acc: 0.2497 - val_loss: -12597599232.0000 - val_acc: 0.2510\n",
            "Epoch 49/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -12859158528.0000 - acc: 0.2497 - val_loss: -13108306944.0000 - val_acc: 0.2510\n",
            "Epoch 50/50\n",
            "2344/2344 [==============================] - 12s 5ms/step - loss: -13375847424.0000 - acc: 0.2497 - val_loss: -13629696000.0000 - val_acc: 0.2510\n"
          ]
        }
      ],
      "source": [
        "history1=model1.fit(X_train,y_train, epochs=50, verbose=True, validation_data=(X_test,y_test), batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVIl4O7tmpgd"
      },
      "source": [
        "2) Drop the \"unsup\" label from the dataset file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXr2W4KUomaf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Keep all rows that do not have unsup as a label\n",
        "df = df.loc[df[\"label\"] != \"unsup\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwEPIolGxeHE",
        "outputId": "072dfd58-740e-42bd-9eac-9491fef08c06",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(['neg', 'pos'], dtype=object)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check that the label column only has 'neg' and 'pos' labels\n",
        "df['label'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU0Ely1Hom3r"
      },
      "source": [
        "3a) Filter the reviews by punctuation characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrjfpQP-pCgt",
        "outputId": "25c83ad0-bad6-472b-af69-31150499ec3d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "# Remove punctuation from the reviews\n",
        "df['review'] = df['review'].str.replace(\"[^a-zA-Z]\", \" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nMGxp_tDUH7n",
        "outputId": "235c4fd4-303e-49e0-a9cf-8ba32367ff98",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-07fcaf9b-157d-4f37-b98f-79d3f681dad1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>Once again Mr  Costner has dragged out a movie...</td>\n",
              "      <td>neg</td>\n",
              "      <td>0_2.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "      <td>This is an example of why the majority of acti...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10000_4.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>test</td>\n",
              "      <td>First of all I hate those moronic rappers  who...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10001_1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "      <td>Not even the Beatles could write songs everyon...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10002_3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>test</td>\n",
              "      <td>Brass pictures  movies is not a fitting word f...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10003_3.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07fcaf9b-157d-4f37-b98f-79d3f681dad1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07fcaf9b-157d-4f37-b98f-79d3f681dad1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07fcaf9b-157d-4f37-b98f-79d3f681dad1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  type                                             review label  \\\n",
              "0           0  test  Once again Mr  Costner has dragged out a movie...   neg   \n",
              "1           1  test  This is an example of why the majority of acti...   neg   \n",
              "2           2  test  First of all I hate those moronic rappers  who...   neg   \n",
              "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
              "4           4  test  Brass pictures  movies is not a fitting word f...   neg   \n",
              "\n",
              "          file  \n",
              "0      0_2.txt  \n",
              "1  10000_4.txt  \n",
              "2  10001_1.txt  \n",
              "3  10002_3.txt  \n",
              "4  10003_3.txt  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5BARqH4pCx9"
      },
      "source": [
        "3b) Filter the reviews by lower case words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30g7gynkpHYj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Turn all the review text into lowercase\n",
        "df['review'] = df['review'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WXRxQ5zNVMI1",
        "outputId": "e38e9d62-4005-4d42-b026-d934b9045b55",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f83b3cbc-0f31-4103-81aa-d75880da3803\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>type</th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "      <th>file</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>test</td>\n",
              "      <td>once again mr  costner has dragged out a movie...</td>\n",
              "      <td>neg</td>\n",
              "      <td>0_2.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>test</td>\n",
              "      <td>this is an example of why the majority of acti...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10000_4.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>test</td>\n",
              "      <td>first of all i hate those moronic rappers  who...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10001_1.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>test</td>\n",
              "      <td>not even the beatles could write songs everyon...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10002_3.txt</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>test</td>\n",
              "      <td>brass pictures  movies is not a fitting word f...</td>\n",
              "      <td>neg</td>\n",
              "      <td>10003_3.txt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f83b3cbc-0f31-4103-81aa-d75880da3803')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f83b3cbc-0f31-4103-81aa-d75880da3803 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f83b3cbc-0f31-4103-81aa-d75880da3803');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  type                                             review label  \\\n",
              "0           0  test  once again mr  costner has dragged out a movie...   neg   \n",
              "1           1  test  this is an example of why the majority of acti...   neg   \n",
              "2           2  test  first of all i hate those moronic rappers  who...   neg   \n",
              "3           3  test  not even the beatles could write songs everyon...   neg   \n",
              "4           4  test  brass pictures  movies is not a fitting word f...   neg   \n",
              "\n",
              "          file  \n",
              "0      0_2.txt  \n",
              "1  10000_4.txt  \n",
              "2  10001_1.txt  \n",
              "3  10002_3.txt  \n",
              "4  10003_3.txt  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWaRHnixpHr-"
      },
      "source": [
        "3c) Filter the reviews by reducing the words to their root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BD6szDlVaHS",
        "outputId": "339a50ec-c5af-427f-eef4-4ec9fa56c173",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89kIV0sypNH3",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Tokenize the review texts by word\n",
        "df['review'] = df['review'].apply(lambda x: word_tokenize(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdVTPX3QWA71",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "porter = PorterStemmer()\n",
        "\n",
        "#Stemming for each Series values\n",
        "df['review'] = df['review'].apply(lambda x: [porter.stem(word) for word in x])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQeto1dJpOED"
      },
      "source": [
        "4) Add embedding layer to the model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJpBgl0BZBNr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Prepare data for embedding layer\n",
        "sentences = df['review'].values\n",
        "y = df['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHHM1sHWdwUj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Identify variables needed for embedding layer\n",
        "max_review_len = max([len(s) for s in sentences])\n",
        "vocab_size = len(tokenizer.word_index)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXcb-JtcmwLg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Reprocess the data\n",
        "X = tokenizer.texts_to_sequences(sentences)\n",
        "X = pad_sequences(X, maxlen=max_review_len)\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coXVtGZ2m2kb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create new training and testing datasets\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNpCWdqGdiVX",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model2 = Sequential()\n",
        "\n",
        "# Add embedding layer to the model\n",
        "model2.add(layers.Embedding(vocab_size, 3, input_length=max_review_len))\n",
        "model2.add(layers.Flatten())\n",
        "\n",
        "# Adjust the dense layer to be compatible with the embedding layer\n",
        "model2.add(layers.Dense(3, activation='relu'))\n",
        "\n",
        "model2.add(layers.Dense(1, activation='sigmoid'))\n",
        "model2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zjw9z0itf0J5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Remake the original model to compare with the embedded layer\n",
        "model3 = Sequential()\n",
        "\n",
        "# Make adjustments to use the new dataset dimensions\n",
        "input_dim = X_train2.shape[1]\n",
        "model3.add(layers.Dense(300,input_dim=input_dim, activation='relu'))\n",
        "model3.add(layers.Dense(1, activation='sigmoid'))\n",
        "model3.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6tbRjtKeBtF",
        "outputId": "e12b0213-cfd0-4fea-8599-e91324b643e6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1172/1172 [==============================] - 11s 8ms/step - loss: 0.6932 - acc: 0.4946 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 2/50\n",
            "1172/1172 [==============================] - 9s 8ms/step - loss: 0.6932 - acc: 0.4972 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 3/50\n",
            "1172/1172 [==============================] - 9s 8ms/step - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 4/50\n",
            "1172/1172 [==============================] - 9s 8ms/step - loss: 0.6932 - acc: 0.4999 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 5/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.5006 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 6/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 7/50\n",
            "1172/1172 [==============================] - 9s 8ms/step - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6933 - val_acc: 0.4968\n",
            "Epoch 8/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 9/50\n",
            "1172/1172 [==============================] - 9s 8ms/step - loss: 0.6932 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 10/50\n",
            "1172/1172 [==============================] - 9s 8ms/step - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6931 - val_acc: 0.4968\n",
            "Epoch 11/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 12/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4982 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 13/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 14/50\n",
            "1172/1172 [==============================] - 9s 8ms/step - loss: 0.6932 - acc: 0.4983 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 15/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 16/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.5004 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 17/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4981 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 18/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.5017 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 19/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4962 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 20/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4958 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 21/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6933 - val_acc: 0.4968\n",
            "Epoch 22/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.5019 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 23/50\n",
            "1172/1172 [==============================] - 9s 8ms/step - loss: 0.6932 - acc: 0.4964 - val_loss: 0.6933 - val_acc: 0.4968\n",
            "Epoch 24/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 25/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 26/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.5023 - val_loss: 0.6931 - val_acc: 0.4968\n",
            "Epoch 27/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4980 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 28/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4971 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 29/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 30/50\n",
            "1172/1172 [==============================] - 10s 9ms/step - loss: 0.6932 - acc: 0.4963 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 31/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4994 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 32/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4975 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 33/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 34/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6931 - acc: 0.5046 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 35/50\n",
            "1172/1172 [==============================] - 10s 9ms/step - loss: 0.6932 - acc: 0.5010 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 36/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6931 - acc: 0.5020 - val_loss: 0.6933 - val_acc: 0.4968\n",
            "Epoch 37/50\n",
            "1172/1172 [==============================] - 10s 9ms/step - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 38/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4993 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 39/50\n",
            "1172/1172 [==============================] - 10s 9ms/step - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 40/50\n",
            "1172/1172 [==============================] - 16s 14ms/step - loss: 0.6932 - acc: 0.4987 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 41/50\n",
            "1172/1172 [==============================] - 14s 12ms/step - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 42/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4997 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 43/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 44/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4965 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 45/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4954 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 46/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.5009 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 47/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.5003 - val_loss: 0.6933 - val_acc: 0.4968\n",
            "Epoch 48/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.5008 - val_loss: 0.6932 - val_acc: 0.4968\n",
            "Epoch 49/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4996 - val_loss: 0.6931 - val_acc: 0.5032\n",
            "Epoch 50/50\n",
            "1172/1172 [==============================] - 10s 8ms/step - loss: 0.6932 - acc: 0.4933 - val_loss: 0.6932 - val_acc: 0.4968\n"
          ]
        }
      ],
      "source": [
        "history2=model2.fit(X_train2,y_train2, epochs=50, verbose=True, validation_data=(X_test2,y_test2), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBNYIaAyYFsQ",
        "outputId": "90bd0d2a-e7bf-4b68-c199-1d256994f778",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1172/1172 [==============================] - 7s 5ms/step - loss: 4.6566 - acc: 0.5050 - val_loss: 0.7439 - val_acc: 0.5091\n",
            "Epoch 2/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6959 - acc: 0.5427 - val_loss: 0.7329 - val_acc: 0.5154\n",
            "Epoch 3/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6713 - acc: 0.5701 - val_loss: 0.7431 - val_acc: 0.5167\n",
            "Epoch 4/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6662 - acc: 0.5800 - val_loss: 0.7434 - val_acc: 0.5159\n",
            "Epoch 5/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6701 - acc: 0.5836 - val_loss: 0.7677 - val_acc: 0.5194\n",
            "Epoch 6/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6767 - acc: 0.5796 - val_loss: 0.7407 - val_acc: 0.5217\n",
            "Epoch 7/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6696 - acc: 0.5814 - val_loss: 0.7677 - val_acc: 0.5170\n",
            "Epoch 8/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6651 - acc: 0.5841 - val_loss: 0.7960 - val_acc: 0.5166\n",
            "Epoch 9/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6603 - acc: 0.5894 - val_loss: 0.7564 - val_acc: 0.5170\n",
            "Epoch 10/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6500 - acc: 0.5956 - val_loss: 0.8619 - val_acc: 0.5130\n",
            "Epoch 11/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6626 - acc: 0.6049 - val_loss: 0.7617 - val_acc: 0.5169\n",
            "Epoch 12/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6329 - acc: 0.6140 - val_loss: 0.8171 - val_acc: 0.5175\n",
            "Epoch 13/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6159 - acc: 0.6253 - val_loss: 0.8402 - val_acc: 0.5209\n",
            "Epoch 14/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6136 - acc: 0.6369 - val_loss: 0.8550 - val_acc: 0.5174\n",
            "Epoch 15/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.6092 - acc: 0.6420 - val_loss: 0.8851 - val_acc: 0.5199\n",
            "Epoch 16/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5937 - acc: 0.6499 - val_loss: 0.8972 - val_acc: 0.5227\n",
            "Epoch 17/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5793 - acc: 0.6608 - val_loss: 0.9437 - val_acc: 0.5211\n",
            "Epoch 18/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5611 - acc: 0.6696 - val_loss: 0.9792 - val_acc: 0.5227\n",
            "Epoch 19/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5604 - acc: 0.6773 - val_loss: 0.9963 - val_acc: 0.5219\n",
            "Epoch 20/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5486 - acc: 0.6849 - val_loss: 1.0819 - val_acc: 0.5250\n",
            "Epoch 21/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5418 - acc: 0.6909 - val_loss: 1.1561 - val_acc: 0.5212\n",
            "Epoch 22/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5314 - acc: 0.6982 - val_loss: 1.1047 - val_acc: 0.5226\n",
            "Epoch 23/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5247 - acc: 0.7051 - val_loss: 1.1666 - val_acc: 0.5211\n",
            "Epoch 24/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5175 - acc: 0.7107 - val_loss: 1.1846 - val_acc: 0.5204\n",
            "Epoch 25/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.5073 - acc: 0.7160 - val_loss: 1.2090 - val_acc: 0.5190\n",
            "Epoch 26/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.4988 - acc: 0.7233 - val_loss: 1.2512 - val_acc: 0.5239\n",
            "Epoch 27/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.4936 - acc: 0.7278 - val_loss: 1.3420 - val_acc: 0.5218\n",
            "Epoch 28/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.4830 - acc: 0.7343 - val_loss: 1.4252 - val_acc: 0.5226\n",
            "Epoch 29/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.4759 - acc: 0.7390 - val_loss: 1.3827 - val_acc: 0.5216\n",
            "Epoch 30/50\n",
            "1172/1172 [==============================] - 6s 6ms/step - loss: 0.4761 - acc: 0.7426 - val_loss: 1.4219 - val_acc: 0.5188\n",
            "Epoch 31/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4631 - acc: 0.7470 - val_loss: 1.5446 - val_acc: 0.5278\n",
            "Epoch 32/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4628 - acc: 0.7507 - val_loss: 1.5568 - val_acc: 0.5246\n",
            "Epoch 33/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4499 - acc: 0.7531 - val_loss: 1.6129 - val_acc: 0.5258\n",
            "Epoch 34/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.4516 - acc: 0.7576 - val_loss: 1.6652 - val_acc: 0.5243\n",
            "Epoch 35/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4436 - acc: 0.7601 - val_loss: 1.8681 - val_acc: 0.5234\n",
            "Epoch 36/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.4405 - acc: 0.7637 - val_loss: 1.6793 - val_acc: 0.5252\n",
            "Epoch 37/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4322 - acc: 0.7667 - val_loss: 1.6348 - val_acc: 0.5248\n",
            "Epoch 38/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4193 - acc: 0.7734 - val_loss: 1.9513 - val_acc: 0.5264\n",
            "Epoch 39/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4199 - acc: 0.7740 - val_loss: 1.9551 - val_acc: 0.5301\n",
            "Epoch 40/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4169 - acc: 0.7777 - val_loss: 2.0162 - val_acc: 0.5277\n",
            "Epoch 41/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.4102 - acc: 0.7807 - val_loss: 2.0399 - val_acc: 0.5263\n",
            "Epoch 42/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.4191 - acc: 0.7819 - val_loss: 2.1003 - val_acc: 0.5264\n",
            "Epoch 43/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4092 - acc: 0.7850 - val_loss: 2.1933 - val_acc: 0.5282\n",
            "Epoch 44/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.3947 - acc: 0.7890 - val_loss: 2.3403 - val_acc: 0.5238\n",
            "Epoch 45/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.4008 - acc: 0.7886 - val_loss: 2.2308 - val_acc: 0.5270\n",
            "Epoch 46/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.3945 - acc: 0.7921 - val_loss: 2.4566 - val_acc: 0.5236\n",
            "Epoch 47/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.3902 - acc: 0.7960 - val_loss: 2.2453 - val_acc: 0.5240\n",
            "Epoch 48/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.3835 - acc: 0.7983 - val_loss: 2.1908 - val_acc: 0.5270\n",
            "Epoch 49/50\n",
            "1172/1172 [==============================] - 6s 5ms/step - loss: 0.3814 - acc: 0.7990 - val_loss: 2.6707 - val_acc: 0.5269\n",
            "Epoch 50/50\n",
            "1172/1172 [==============================] - 7s 6ms/step - loss: 0.3817 - acc: 0.7993 - val_loss: 2.6486 - val_acc: 0.5280\n"
          ]
        }
      ],
      "source": [
        "history3=model3.fit(X_train2,y_train2, epochs=50, verbose=True, validation_data=(X_test2,y_test2), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GOws00NjrBh",
        "outputId": "24e25007-07c5-4d77-9b8f-0a11f248ef24",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 1s 4ms/step - loss: 0.6932 - acc: 0.4968\n",
            "391/391 [==============================] - 1s 3ms/step - loss: 2.6486 - acc: 0.5280\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the two models to identify their accuracy\n",
        "pred1 = model2.evaluate(X_test2, y_test2)\n",
        "pred2 = model3.evaluate(X_test2, y_test2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJVLIlILkJYT",
        "outputId": "56c73d93-8c59-4bda-d7c9-964624894909",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedded Model Accuracy:  0.4968000054359436\n",
            "Original Model Accuracy:  0.527999997138977\n"
          ]
        }
      ],
      "source": [
        "# Display their accuracies\n",
        "print('Embedded Model Accuracy: ', pred1[1])\n",
        "print('Original Model Accuracy: ', pred2[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP89Y5xclMKC"
      },
      "source": [
        "Comparing the results of the two models, we can see that adding the embedded layer did not improve the accuracy. As we can see with the results above, adding the embedding layer caused a decrease of roughly 0.03 to the accuracy. Looking at the loss values, we can see that the original model has a larger loss value than the embedded model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bXVWWVUpSQY"
      },
      "source": [
        "5) \n",
        "\n",
        "With the original model, we can see that it is suffering from underfitting since it has poor performace in both the fitting and evaluating processes. \n",
        "\n",
        "Based on the data seen in the fitting process and the evaluating process, we can see that the embedded layer model is suffering from underfitting since it is achieving low accuracies during training and evaluating. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOQyDKEipVqJ"
      },
      "source": [
        "6) Apply code on 20_newsgroup dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTJFsE1iq8X5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY8dZTl_ro8U",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Define categories to use from the dataset\n",
        "categories = ['alt.atheism', 'soc.religion.christian']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', shuffle=True, categories=categories)\n",
        "\n",
        "# Define variables for the embedding layer\n",
        "MAX_LEN = 1000\n",
        "max_features = 2000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez387HTBsEny",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Identify the x and y data\n",
        "X = newsgroups_train.data\n",
        "y = newsgroups_train.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNRMtJVPslRs",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create sequences from the text\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrSVYtl5fvTH",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Pad the sequences and label encode the labels\n",
        "X = pad_sequences(X, maxlen=MAX_LEN)\n",
        "y = le.fit_transform(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLck_WiDgdqW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Create new training and testing datasets\n",
        "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.25, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8zvtQDHtmqO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Remake the embedding model to work with the new dataset\n",
        "model4 = Sequential()\n",
        "\n",
        "model4.add(layers.Embedding(max_features, 3, input_length=X.shape[1]))\n",
        "model4.add(layers.Flatten())\n",
        "model4.add(layers.Dense(3, activation='relu'))\n",
        "\n",
        "model4.add(layers.Dense(1, activation='sigmoid'))\n",
        "model4.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HpA4XYOtK4N",
        "outputId": "6dfea7d0-46f5-46bf-a55d-49b4d820cd97",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "26/26 [==============================] - 1s 16ms/step - loss: 0.6905 - acc: 0.5562 - val_loss: 0.6871 - val_acc: 0.5444\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.6812 - acc: 0.5637 - val_loss: 0.6841 - val_acc: 0.5481\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.6707 - acc: 0.5661 - val_loss: 0.6794 - val_acc: 0.5593\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.6587 - acc: 0.5674 - val_loss: 0.6828 - val_acc: 0.5556\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.6475 - acc: 0.5711 - val_loss: 0.6709 - val_acc: 0.5630\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.6251 - acc: 0.5711 - val_loss: 0.6625 - val_acc: 0.5630\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.5877 - acc: 0.6836 - val_loss: 0.6443 - val_acc: 0.6148\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.5233 - acc: 0.8616 - val_loss: 0.6151 - val_acc: 0.6704\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.4437 - acc: 0.9110 - val_loss: 0.5717 - val_acc: 0.7333\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.3625 - acc: 0.9382 - val_loss: 0.5244 - val_acc: 0.7852\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.2845 - acc: 0.9617 - val_loss: 0.4852 - val_acc: 0.7889\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.2218 - acc: 0.9753 - val_loss: 0.4443 - val_acc: 0.8444\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.1705 - acc: 0.9876 - val_loss: 0.4064 - val_acc: 0.8556\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.1355 - acc: 0.9864 - val_loss: 0.3855 - val_acc: 0.8704\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.1083 - acc: 0.9901 - val_loss: 0.3673 - val_acc: 0.8630\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0890 - acc: 0.9938 - val_loss: 0.3543 - val_acc: 0.8593\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0748 - acc: 0.9938 - val_loss: 0.3447 - val_acc: 0.8630\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0611 - acc: 0.9951 - val_loss: 0.3407 - val_acc: 0.8667\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0518 - acc: 0.9975 - val_loss: 0.3322 - val_acc: 0.8593\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0445 - acc: 0.9963 - val_loss: 0.3239 - val_acc: 0.8667\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0381 - acc: 0.9975 - val_loss: 0.3237 - val_acc: 0.8704\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0331 - acc: 0.9975 - val_loss: 0.3212 - val_acc: 0.8704\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0285 - acc: 0.9975 - val_loss: 0.3212 - val_acc: 0.8704\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0254 - acc: 0.9975 - val_loss: 0.3235 - val_acc: 0.8481\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0230 - acc: 0.9975 - val_loss: 0.3212 - val_acc: 0.8630\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0192 - acc: 0.9988 - val_loss: 0.3180 - val_acc: 0.8630\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0171 - acc: 0.9988 - val_loss: 0.3152 - val_acc: 0.8667\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0154 - acc: 0.9988 - val_loss: 0.3158 - val_acc: 0.8630\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0139 - acc: 0.9988 - val_loss: 0.3163 - val_acc: 0.8556\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0129 - acc: 0.9988 - val_loss: 0.3144 - val_acc: 0.8630\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.3179 - val_acc: 0.8593\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.3169 - val_acc: 0.8556\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0097 - acc: 0.9988 - val_loss: 0.3143 - val_acc: 0.8593\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0086 - acc: 1.0000 - val_loss: 0.3199 - val_acc: 0.8630\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.3206 - val_acc: 0.8630\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.3203 - val_acc: 0.8593\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.3170 - val_acc: 0.8519\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0062 - acc: 1.0000 - val_loss: 0.3260 - val_acc: 0.8667\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.3191 - val_acc: 0.8556\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.3204 - val_acc: 0.8556\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.3198 - val_acc: 0.8556\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.3209 - val_acc: 0.8556\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.3214 - val_acc: 0.8556\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.3231 - val_acc: 0.8556\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 0s 9ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.3245 - val_acc: 0.8519\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.3238 - val_acc: 0.8519\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 0s 8ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.3278 - val_acc: 0.8519\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.3278 - val_acc: 0.8556\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.3268 - val_acc: 0.8519\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 0s 10ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.3275 - val_acc: 0.8519\n"
          ]
        }
      ],
      "source": [
        "history4=model4.fit(X_train3,y_train3, epochs=50, verbose=True, validation_data=(X_test3,y_test3), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Id5OxOrsTNq",
        "outputId": "14afc81b-88c8-4cab-b182-30b84b8f8ff9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3275 - acc: 0.8519\n",
            "Embedded Model Accuracy:  0.8518518805503845\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "pred3 = model4.evaluate(X_test3, y_test3)\n",
        "\n",
        "# Display accuracy\n",
        "print('Embedded Model Accuracy: ', pred3[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPvZ2IpWwEFQ"
      },
      "source": [
        "By using the 20_newsgroup dataset, we can see that the model is suffering from overfitting since the model does well with the training data, but does not perform well with the test data. To help prevent this overfitting, we could use the early stopping callback to monitor the model's performance and stop training when performance degrades. We could also use the shuffle parameter in the fit method to shuffle the data while fitting the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spxc3eT7q9BS"
      },
      "source": [
        "7) Predict over one sample of data and check its prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJqKxlzWwy8L",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Predict and identify the class of the sample\n",
        "pred7 = model4.predict(X_test3[3].reshape(1, 1000))\n",
        "max = np.argmax(pred7, axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RGnzdNVx1Nz",
        "outputId": "0ffc969b-ca01-43f5-f585-e9fa3142835f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYeF4pREx9xW",
        "outputId": "39edff77-3742-41f7-e76a-4ba4b70dad49",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Identify the actual class of the sample\n",
        "y_test3[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjID4iKl05zd"
      },
      "source": [
        "E.C. Add the convolution and max pooling layer to the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYma9NxA1KKC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Remake the embedding layer model with the new layers\n",
        "model5 = Sequential()\n",
        "\n",
        "model5.add(layers.Embedding(max_features, 3, input_length=X.shape[1]))\n",
        "\n",
        "# Add the convolution layer\n",
        "model5.add(layers.Conv1D(32, 1000, padding='same'))\n",
        "model5.add(layers.Activation('relu'))\n",
        "\n",
        "# Add the max pooling layer\n",
        "model5.add(layers.MaxPooling1D(pool_size=1000))\n",
        "\n",
        "model5.add(layers.Flatten())\n",
        "\n",
        "model5.add(layers.Dense(3, activation='relu'))\n",
        "model5.add(layers.Dense(1, activation='sigmoid'))\n",
        "model5.compile(loss='binary_crossentropy',optimizer='adam',metrics=['acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP2n7Ch72_7d",
        "outputId": "99840dea-73e0-45ba-fd47-311e8f6b2b89",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "26/26 [==============================] - 2s 48ms/step - loss: 0.6936 - acc: 0.5278 - val_loss: 0.6924 - val_acc: 0.5519\n",
            "Epoch 2/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.6919 - acc: 0.5562 - val_loss: 0.6917 - val_acc: 0.5519\n",
            "Epoch 3/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.6911 - acc: 0.5624 - val_loss: 0.6912 - val_acc: 0.5519\n",
            "Epoch 4/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.6904 - acc: 0.5637 - val_loss: 0.6906 - val_acc: 0.5519\n",
            "Epoch 5/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.6894 - acc: 0.5661 - val_loss: 0.6901 - val_acc: 0.5519\n",
            "Epoch 6/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.6877 - acc: 0.5760 - val_loss: 0.6897 - val_acc: 0.5556\n",
            "Epoch 7/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.6848 - acc: 0.5797 - val_loss: 0.6891 - val_acc: 0.5556\n",
            "Epoch 8/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.6774 - acc: 0.5921 - val_loss: 0.6863 - val_acc: 0.5778\n",
            "Epoch 9/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.6480 - acc: 0.7528 - val_loss: 0.6602 - val_acc: 0.6037\n",
            "Epoch 10/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.5672 - acc: 0.8220 - val_loss: 0.5802 - val_acc: 0.7519\n",
            "Epoch 11/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.4562 - acc: 0.9110 - val_loss: 0.4997 - val_acc: 0.8296\n",
            "Epoch 12/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.3866 - acc: 0.9629 - val_loss: 0.5072 - val_acc: 0.6963\n",
            "Epoch 13/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.3520 - acc: 0.9753 - val_loss: 0.4666 - val_acc: 0.8074\n",
            "Epoch 14/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.3287 - acc: 0.9926 - val_loss: 0.4440 - val_acc: 0.8222\n",
            "Epoch 15/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.3137 - acc: 0.9963 - val_loss: 0.4358 - val_acc: 0.8185\n",
            "Epoch 16/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.3024 - acc: 1.0000 - val_loss: 0.4617 - val_acc: 0.8111\n",
            "Epoch 17/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.2932 - acc: 1.0000 - val_loss: 0.4276 - val_acc: 0.8370\n",
            "Epoch 18/50\n",
            "26/26 [==============================] - 1s 39ms/step - loss: 0.2844 - acc: 1.0000 - val_loss: 0.4257 - val_acc: 0.8370\n",
            "Epoch 19/50\n",
            "26/26 [==============================] - 1s 39ms/step - loss: 0.2765 - acc: 1.0000 - val_loss: 0.4312 - val_acc: 0.8333\n",
            "Epoch 20/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.2697 - acc: 1.0000 - val_loss: 0.4023 - val_acc: 0.8630\n",
            "Epoch 21/50\n",
            "26/26 [==============================] - 1s 39ms/step - loss: 0.2628 - acc: 1.0000 - val_loss: 0.4163 - val_acc: 0.8519\n",
            "Epoch 22/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.2564 - acc: 1.0000 - val_loss: 0.3883 - val_acc: 0.8593\n",
            "Epoch 23/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.2501 - acc: 1.0000 - val_loss: 0.4343 - val_acc: 0.8333\n",
            "Epoch 24/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.2445 - acc: 1.0000 - val_loss: 0.4363 - val_acc: 0.8370\n",
            "Epoch 25/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.2389 - acc: 1.0000 - val_loss: 0.4004 - val_acc: 0.8519\n",
            "Epoch 26/50\n",
            "26/26 [==============================] - 1s 39ms/step - loss: 0.2341 - acc: 1.0000 - val_loss: 0.3819 - val_acc: 0.8704\n",
            "Epoch 27/50\n",
            "26/26 [==============================] - 1s 39ms/step - loss: 0.2286 - acc: 1.0000 - val_loss: 0.4518 - val_acc: 0.8259\n",
            "Epoch 28/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.2249 - acc: 1.0000 - val_loss: 0.3769 - val_acc: 0.8667\n",
            "Epoch 29/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.2191 - acc: 1.0000 - val_loss: 0.3980 - val_acc: 0.8519\n",
            "Epoch 30/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.2145 - acc: 1.0000 - val_loss: 0.4010 - val_acc: 0.8556\n",
            "Epoch 31/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.2100 - acc: 1.0000 - val_loss: 0.4201 - val_acc: 0.8481\n",
            "Epoch 32/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.2058 - acc: 1.0000 - val_loss: 0.4264 - val_acc: 0.8519\n",
            "Epoch 33/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.2017 - acc: 1.0000 - val_loss: 0.3798 - val_acc: 0.8630\n",
            "Epoch 34/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.1976 - acc: 1.0000 - val_loss: 0.3851 - val_acc: 0.8630\n",
            "Epoch 35/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.1937 - acc: 1.0000 - val_loss: 0.4264 - val_acc: 0.8444\n",
            "Epoch 36/50\n",
            "26/26 [==============================] - 1s 38ms/step - loss: 0.1900 - acc: 1.0000 - val_loss: 0.4126 - val_acc: 0.8556\n",
            "Epoch 37/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.1863 - acc: 1.0000 - val_loss: 0.4163 - val_acc: 0.8481\n",
            "Epoch 38/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.1827 - acc: 1.0000 - val_loss: 0.3972 - val_acc: 0.8481\n",
            "Epoch 39/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.1792 - acc: 1.0000 - val_loss: 0.4223 - val_acc: 0.8444\n",
            "Epoch 40/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.1758 - acc: 1.0000 - val_loss: 0.4276 - val_acc: 0.8444\n",
            "Epoch 41/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.1725 - acc: 1.0000 - val_loss: 0.4080 - val_acc: 0.8407\n",
            "Epoch 42/50\n",
            "26/26 [==============================] - 1s 35ms/step - loss: 0.1693 - acc: 1.0000 - val_loss: 0.4267 - val_acc: 0.8444\n",
            "Epoch 43/50\n",
            "26/26 [==============================] - 1s 35ms/step - loss: 0.1662 - acc: 1.0000 - val_loss: 0.3810 - val_acc: 0.8519\n",
            "Epoch 44/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.1631 - acc: 1.0000 - val_loss: 0.3825 - val_acc: 0.8630\n",
            "Epoch 45/50\n",
            "26/26 [==============================] - 1s 35ms/step - loss: 0.1601 - acc: 1.0000 - val_loss: 0.3828 - val_acc: 0.8667\n",
            "Epoch 46/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.1572 - acc: 1.0000 - val_loss: 0.3980 - val_acc: 0.8556\n",
            "Epoch 47/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.1544 - acc: 1.0000 - val_loss: 0.4062 - val_acc: 0.8556\n",
            "Epoch 48/50\n",
            "26/26 [==============================] - 1s 37ms/step - loss: 0.1517 - acc: 1.0000 - val_loss: 0.3843 - val_acc: 0.8630\n",
            "Epoch 49/50\n",
            "26/26 [==============================] - 1s 35ms/step - loss: 0.1490 - acc: 1.0000 - val_loss: 0.4128 - val_acc: 0.8519\n",
            "Epoch 50/50\n",
            "26/26 [==============================] - 1s 36ms/step - loss: 0.1464 - acc: 1.0000 - val_loss: 0.3850 - val_acc: 0.8593\n"
          ]
        }
      ],
      "source": [
        "history5=model5.fit(X_train3,y_train3, epochs=50, verbose=True, validation_data=(X_test3,y_test3), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ecvj_PnD3O4E",
        "outputId": "8d89cbe4-6f40-4f67-d9ac-f9d0e84d6b66",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 0s 15ms/step - loss: 0.3850 - acc: 0.8593\n",
            "Embedded Model Accuracy:  0.8592592477798462\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "pred5 = model5.evaluate(X_test3, y_test3)\n",
        "\n",
        "# Display accuracy\n",
        "print('Embedded Model Accuracy: ', pred5[1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Yap_Yoon_ICP_11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}